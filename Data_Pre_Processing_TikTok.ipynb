{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4c8085-527b-4680-894f-b1ca1f6b6b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nLtk in c:\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\programs\\python\\python312\\lib\\site-packages (from click->nLtk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nLtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4612a2-b918-457b-8e71-d1c4890270a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from langdetect import detect, DetectorFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e696f4ea-28be-4202-8b4b-1632a9792537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download NLTK stopwords and punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfaac7be-e955-494b-aeaa-fc8666cfb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stop words\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1c90e2-5ea2-4583-b514-18ebc6e229d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file of dataset and select only the 'Comments' column\n",
    "data = pd.read_csv(r'C:\\Users\\paris\\OneDrive\\thesis\\collect data\\TikTok-comments.csv')[['Comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc58031-7f81-4428-8a4d-dbb365368bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Comments\n",
      "0  Jojoba oil instead of olive oil ðŸ™ˆ jojoba is me...\n",
      "1        Do you just throw them in the washer after?\n",
      "2  The magic makeup eraser towels last for years ...\n",
      "3                      The makeup eraser clothâ€¦10/10\n",
      "4  esthetician hereâ€” try almond oil or jojoba oil...\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "413132fe-7b15-495f-b16f-c59db5fe5893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: Index(['Comments'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Verify the existing columns\n",
    "print(\"Columns in DataFrame:\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b03c48-41f7-427b-aede-9de2fe6bafac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization, reduce words to their base or root form\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ae3aa3-d2e7-4eb1-871d-f90a8d82155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comments  \\\n",
      "0     Jojoba oil instead of olive oil ðŸ™ˆ jojoba is me...   \n",
      "1           Do you just throw them in the washer after?   \n",
      "2     The magic makeup eraser towels last for years ...   \n",
      "3                         The makeup eraser clothâ€¦10/10   \n",
      "4     esthetician hereâ€” try almond oil or jojoba oil...   \n",
      "...                                                 ...   \n",
      "2358                                 Thank you yes!!!!!   \n",
      "2359                          We have new mascara too!!   \n",
      "2360                                  Pacifica is ITâœ¨âœ¨âœ¨   \n",
      "2361  Elf concealer has forever chemicals in it if s...   \n",
      "2362            could you please share alternatives? <3   \n",
      "\n",
      "                                       Cleaned_Comments  \n",
      "0     jojoba oil instead olive oil jojoba meant clea...  \n",
      "1                                  throw washer after ?  \n",
      "2      magic makeup eraser towels last years need water  \n",
      "3                                   makeup eraser cloth  \n",
      "4         esthetician try almond oil jojoba oil instead  \n",
      "...                                                 ...  \n",
      "2358                                          thank yes  \n",
      "2359                                                     \n",
      "2360                                           pacifica  \n",
      "2361      elf concealer forever chemicals stay away elf  \n",
      "2362                  could please share alternatives ?  \n",
      "\n",
      "[2363 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define custom stop words excluding sentiment-critical words like \"not\"\n",
    "\n",
    "custom_stop_words = stop_words.union({\"hii\", \"day\", \"bar\", \"ill\", \"weather\", \"new\", \"sound\", \"see\", \"well\", \"im\", \"ive\", \n",
    "   \"also\", \"youre\", \"win\", \"thats\", \"send\", \"dm\", \"mascara\", \"laurence\", \"nars\", \"pp\", \"ao\", \"de\", \"e\", \"xx\", \"xxx\", \"get\",\n",
    "   \"quel\", \"savadogo\", \"banilla\", \"w\", \"hiii\", \"hiiiii\"\n",
    " \n",
    "}).difference({\"able\", \"affected\", \"affecting\", \"affects\", \"after\", \"again\", \"against\", \"ain't\", \"all\", \"always\",\n",
    "    \"anyone\", \"anybody\", \"aren't\", \"awfully\", \"best\", \"better\", \"definitely\", \"end\", \"ending\", \"fairly\", \"will\", \"have this\",\n",
    "    \"hopefully\", \"however\", \"but\", \"importance\", \"important\", \"information\", \"instead\", \"interest\", \"isn't\", \"have product\",\n",
    "    \"less\", \"like\", \"liked\", \"mustn't\", \"necessary\", \"needn't\", \"ok\", \"okay\", \"shouldn't\", \"suggest\", \n",
    "    \"unfortunately\", \"use\", \"used\", \"useful\", \"using\", \"want\", \"wants\", \"wasn't\", \"won't\", \"wouldn't\", \"not\", \"no\", \"never\"})\n",
    "custom_stop_words = {word.lower() for word in custom_stop_words}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map Penn Treebank POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "#1. remove URLs, Mentions-Tags-\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\s+|www\\s+', '', text)\n",
    "#2. remove special charachters and numbers,but keep question marks-sentiment-critical\n",
    "    text = re.sub(r'[^a-zA-Z\\s?]','',text)\n",
    "#3. convert to lowercase- Text normalization\n",
    "    text = text.lower()\n",
    "#4. Tokenizw and remove stop words\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_words = [word for word in tokens if word not in custom_stop_words]\n",
    "    # Join words back into one string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "#Apply function to each text in the DataFrame\n",
    "data['Cleaned_Comments'] = data['Comments'].apply(clean_text)\n",
    "print(data[['Comments', 'Cleaned_Comments']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3fe236-b690-4d7e-8a93-1d583f08779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['which', 'w', 've', 'from', 'as', 'shouldn', 'their', 'nor', 'more', 'for', 'where', 'shan', 'his', 'because', 'mustn', 'only', 'haven', \"shan't\", 'get', 'hiii', 'how', 'any', 'some', 'weren', 'who', 'o', \"you'd\", 'see', 'couldn', 'had']\n"
     ]
    }
   ],
   "source": [
    "# Print the first few stop words to confirm\n",
    "print(list(custom_stop_words)[:30])  # Debugging: Check stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66935321-f8ee-4f16-b07f-426a212a1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "# Lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize cleaned text\n",
    "    pos_tags = pos_tag(tokens)  # Get POS tags\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)  # Return lemmatized text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf23266-429e-4a80-a694-b3d5cc5872df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comments  \\\n",
      "0     Jojoba oil instead of olive oil ðŸ™ˆ jojoba is me...   \n",
      "1           Do you just throw them in the washer after?   \n",
      "2     The magic makeup eraser towels last for years ...   \n",
      "3                         The makeup eraser clothâ€¦10/10   \n",
      "4     esthetician hereâ€” try almond oil or jojoba oil...   \n",
      "...                                                 ...   \n",
      "2358                                 Thank you yes!!!!!   \n",
      "2359                          We have new mascara too!!   \n",
      "2360                                  Pacifica is ITâœ¨âœ¨âœ¨   \n",
      "2361  Elf concealer has forever chemicals in it if s...   \n",
      "2362            could you please share alternatives? <3   \n",
      "\n",
      "                                       Cleaned_Comments  \\\n",
      "0     jojoba oil instead olive oil jojoba meant clea...   \n",
      "1                                  throw washer after ?   \n",
      "2      magic makeup eraser towels last years need water   \n",
      "3                                   makeup eraser cloth   \n",
      "4         esthetician try almond oil jojoba oil instead   \n",
      "...                                                 ...   \n",
      "2358                                          thank yes   \n",
      "2359                                                      \n",
      "2360                                           pacifica   \n",
      "2361      elf concealer forever chemicals stay away elf   \n",
      "2362                  could please share alternatives ?   \n",
      "\n",
      "                                    Lemmatized_Comments  \n",
      "0     jojoba oil instead olive oil jojoba mean clean...  \n",
      "1                                  throw washer after ?  \n",
      "2        magic makeup eraser towel last year need water  \n",
      "3                                   makeup eraser cloth  \n",
      "4         esthetician try almond oil jojoba oil instead  \n",
      "...                                                 ...  \n",
      "2358                                          thank yes  \n",
      "2359                                                     \n",
      "2360                                           pacifica  \n",
      "2361       elf concealer forever chemical stay away elf  \n",
      "2362                   could please share alternative ?  \n",
      "\n",
      "[2363 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data['Cleaned_Comments'] = data['Comments'].apply(clean_text)\n",
    "data['Lemmatized_Comments'] = data['Cleaned_Comments'].apply(lemmatize_text)\n",
    "\n",
    "print(data[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f310dfb2-6a86-4a08-8611-4b0318733a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and keep only English comments\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False  # Default to non-English if detection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a12a29a1-4900-4199-8e36-f1bd3145528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comments  \\\n",
      "0     Jojoba oil instead of olive oil ðŸ™ˆ jojoba is me...   \n",
      "1           Do you just throw them in the washer after?   \n",
      "2     The magic makeup eraser towels last for years ...   \n",
      "3                         The makeup eraser clothâ€¦10/10   \n",
      "4     esthetician hereâ€” try almond oil or jojoba oil...   \n",
      "...                                                 ...   \n",
      "2357                                        I love elf.   \n",
      "2358                                 Thank you yes!!!!!   \n",
      "2360                                  Pacifica is ITâœ¨âœ¨âœ¨   \n",
      "2361  Elf concealer has forever chemicals in it if s...   \n",
      "2362            could you please share alternatives? <3   \n",
      "\n",
      "                                       Cleaned_Comments  \\\n",
      "0     jojoba oil instead olive oil jojoba meant clea...   \n",
      "1                                  throw washer after ?   \n",
      "2      magic makeup eraser towels last years need water   \n",
      "3                                   makeup eraser cloth   \n",
      "4         esthetician try almond oil jojoba oil instead   \n",
      "...                                                 ...   \n",
      "2357                                           love elf   \n",
      "2358                                          thank yes   \n",
      "2360                                           pacifica   \n",
      "2361      elf concealer forever chemicals stay away elf   \n",
      "2362                  could please share alternatives ?   \n",
      "\n",
      "                                    Lemmatized_Comments  \n",
      "0     jojoba oil instead olive oil jojoba mean clean...  \n",
      "1                                  throw washer after ?  \n",
      "2        magic makeup eraser towel last year need water  \n",
      "3                                   makeup eraser cloth  \n",
      "4         esthetician try almond oil jojoba oil instead  \n",
      "...                                                 ...  \n",
      "2357                                           love elf  \n",
      "2358                                          thank yes  \n",
      "2360                                           pacifica  \n",
      "2361       elf concealer forever chemical stay away elf  \n",
      "2362                   could please share alternative ?  \n",
      "\n",
      "[2259 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning and lemmatization to the DataFrame\n",
    "data['Cleaned_Comments'] = data['Comments'].apply(clean_text)\n",
    "\n",
    "#  Remove rows containing only question marks or empty strings\n",
    "data = data[~data['Cleaned_Comments'].str.fullmatch(r'[\\s?]*')]  # Remove rows with only ?, whitespace, or empty\n",
    "\n",
    "data['Lemmatized_Comments'] = data['Cleaned_Comments'].apply(lemmatize_text)\n",
    "\n",
    "print(data[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b069a150-bb45-4c50-9084-7c420d46d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clean comments\n",
    "#data['Cleaned_Comments'] = data['Comments'].apply(clean_text)\n",
    "\n",
    "# Remove rows containing only question marks or empty strings\n",
    "#data = data[~data['Cleaned_Comments'].str.fullmatch(r'[\\s?]*')]  # Remove rows with only ?, whitespace, or empty\n",
    "\n",
    "# Remove non-English comments\n",
    "#data = data[data['Cleaned_Comments'].apply(is_english)]\n",
    "\n",
    "#  Lemmatize comments\n",
    "#data['Lemmatized_Comments'] = data['Cleaned_Comments'].apply(lemmatize_text)\n",
    "\n",
    "# Display results\n",
    "#print(data[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99a662f4-0d95-4c3c-b498-586d8851468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows based on lemmatized comments: 8\n",
      "                           Comments         Cleaned_Comments  \\\n",
      "210         Good to know thank you!          good know thank   \n",
      "1140              Thanks for this!!                   thanks   \n",
      "1372      Is Clinique cruelty free?  clinique cruelty free ?   \n",
      "1469  Is the ordinary cruelty free?  ordinary cruelty free ?   \n",
      "1635                            Yep                      yep   \n",
      "1860                       Amazing!                  amazing   \n",
      "2111        So glad to hear that! ðŸ’š                glad hear   \n",
      "2212                      thank you                    thank   \n",
      "\n",
      "          Lemmatized_Comments  \n",
      "210           good know thank  \n",
      "1140                   thanks  \n",
      "1372  clinique cruelty free ?  \n",
      "1469  ordinary cruelty free ?  \n",
      "1635                      yep  \n",
      "1860                    amaze  \n",
      "2111                glad hear  \n",
      "2212                    thank  \n"
     ]
    }
   ],
   "source": [
    "# Identify duplicate rows\n",
    "duplicates = data[data.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicate rows based on lemmatized comments: {duplicates.shape[0]}\")\n",
    "print(duplicates[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa0dbc79-8659-43cd-8141-937cb54cd244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cleaned DataFrame (after removing duplicates but keeping single-word comments): (2237, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates but keep single-word comments\n",
    "data_cleaned = data[~(data.duplicated(subset=['Lemmatized_Comments']) & \n",
    "                      (data['Lemmatized_Comments'].str.split().str.len() > 1))]\n",
    "\n",
    "# Check the shape of the cleaned DataFrame\n",
    "print(f\"Shape of the cleaned DataFrame (after removing duplicates but keeping single-word comments): {data_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c396c6-7f71-4061-b7f9-699359ce6a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comments  \\\n",
      "0     Jojoba oil instead of olive oil ðŸ™ˆ jojoba is me...   \n",
      "1           Do you just throw them in the washer after?   \n",
      "2     The magic makeup eraser towels last for years ...   \n",
      "3                         The makeup eraser clothâ€¦10/10   \n",
      "4     esthetician hereâ€” try almond oil or jojoba oil...   \n",
      "...                                                 ...   \n",
      "2357                                        I love elf.   \n",
      "2358                                 Thank you yes!!!!!   \n",
      "2360                                  Pacifica is ITâœ¨âœ¨âœ¨   \n",
      "2361  Elf concealer has forever chemicals in it if s...   \n",
      "2362            could you please share alternatives? <3   \n",
      "\n",
      "                                       Cleaned_Comments  \\\n",
      "0     jojoba oil instead olive oil jojoba meant clea...   \n",
      "1                                  throw washer after ?   \n",
      "2      magic makeup eraser towels last years need water   \n",
      "3                                   makeup eraser cloth   \n",
      "4         esthetician try almond oil jojoba oil instead   \n",
      "...                                                 ...   \n",
      "2357                                           love elf   \n",
      "2358                                          thank yes   \n",
      "2360                                           pacifica   \n",
      "2361      elf concealer forever chemicals stay away elf   \n",
      "2362                  could please share alternatives ?   \n",
      "\n",
      "                                    Lemmatized_Comments  \n",
      "0     jojoba oil instead olive oil jojoba mean clean...  \n",
      "1                                  throw washer after ?  \n",
      "2        magic makeup eraser towel last year need water  \n",
      "3                                   makeup eraser cloth  \n",
      "4         esthetician try almond oil jojoba oil instead  \n",
      "...                                                 ...  \n",
      "2357                                           love elf  \n",
      "2358                                          thank yes  \n",
      "2360                                           pacifica  \n",
      "2361       elf concealer forever chemical stay away elf  \n",
      "2362                   could please share alternative ?  \n",
      "\n",
      "[2259 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'Lemmatized_Comments' is empty or only contains whitespace, but avoid reformatting issues\n",
    "data = data[data['Lemmatized_Comments'].astype(bool)]  # Keep only rows with non-empty strings\n",
    "\n",
    "# Check the resulting DataFrame to ensure formatting is preserved\n",
    "print(data[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0086738b-f72c-4ab0-a1de-0e29fdeb6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and store non-English rows\n",
    "#non_english_rows = data[~data['Cleaned_Comments'].apply(is_english)]\n",
    "\n",
    "# Display non-English rows\n",
    "#print(non_english_rows[['Comments', 'Cleaned_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "267cecc9-d83c-42e4-add8-318f74712b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\paris\\OneDrive\\thesis\\collect data\\Cleaned_TikTok_comments3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a2fde42-b8af-49da-b02e-a9dcde623bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Comments  \\\n",
      "0     Jojoba oil instead of olive oil ðŸ™ˆ jojoba is me...   \n",
      "1           Do you just throw them in the washer after?   \n",
      "2     The magic makeup eraser towels last for years ...   \n",
      "3                         The makeup eraser clothâ€¦10/10   \n",
      "4     esthetician hereâ€” try almond oil or jojoba oil...   \n",
      "...                                                 ...   \n",
      "2357                                        I love elf.   \n",
      "2358                                 Thank you yes!!!!!   \n",
      "2360                                  Pacifica is ITâœ¨âœ¨âœ¨   \n",
      "2361  Elf concealer has forever chemicals in it if s...   \n",
      "2362            could you please share alternatives? <3   \n",
      "\n",
      "                                       Cleaned_Comments  \\\n",
      "0     jojoba oil instead olive oil jojoba meant clea...   \n",
      "1                                  throw washer after ?   \n",
      "2      magic makeup eraser towels last years need water   \n",
      "3                                   makeup eraser cloth   \n",
      "4         esthetician try almond oil jojoba oil instead   \n",
      "...                                                 ...   \n",
      "2357                                           love elf   \n",
      "2358                                          thank yes   \n",
      "2360                                           pacifica   \n",
      "2361      elf concealer forever chemicals stay away elf   \n",
      "2362                  could please share alternatives ?   \n",
      "\n",
      "                                    Lemmatized_Comments  \\\n",
      "0     jojoba oil instead olive oil jojoba mean clean...   \n",
      "1                                  throw washer after ?   \n",
      "2        magic makeup eraser towel last year need water   \n",
      "3                                   makeup eraser cloth   \n",
      "4         esthetician try almond oil jojoba oil instead   \n",
      "...                                                 ...   \n",
      "2357                                           love elf   \n",
      "2358                                          thank yes   \n",
      "2360                                           pacifica   \n",
      "2361       elf concealer forever chemical stay away elf   \n",
      "2362                   could please share alternative ?   \n",
      "\n",
      "                 adjectives  \\\n",
      "0     [jojoba, olive, wont]   \n",
      "1                        []   \n",
      "2             [magic, last]   \n",
      "3                        []   \n",
      "4             [esthetician]   \n",
      "...                     ...   \n",
      "2357                     []   \n",
      "2358                     []   \n",
      "2360                     []   \n",
      "2361             [chemical]   \n",
      "2362          [alternative]   \n",
      "\n",
      "                                                  nouns              verbs  \n",
      "0     [oil, oil, jojoba, mean, cleanse, pore, clog, ...                 []  \n",
      "1                                       [throw, washer]                 []  \n",
      "2                  [makeup, eraser, towel, year, water]             [need]  \n",
      "3                               [makeup, eraser, cloth]                 []  \n",
      "4                       [try, almond, oil, jojoba, oil]                 []  \n",
      "...                                                 ...                ...  \n",
      "2357                                              [elf]             [love]  \n",
      "2358                                              [yes]            [thank]  \n",
      "2360                                         [pacifica]                 []  \n",
      "2361                                                 []  [concealer, stay]  \n",
      "2362                                            [share]           [please]  \n",
      "\n",
      "[2259 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract words by part of speech (POS)\n",
    "def extract_pos(text, pos_list):\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tagged_tokens = pos_tag(tokens)  # Part-of-speech tagging\n",
    "    return [word for word, tag in tagged_tokens if tag in pos_list]\n",
    "\n",
    "# Define POS tags for adjectives, nouns, and verbs\n",
    "adjective_tags = ['JJ', 'JJR', 'JJS']\n",
    "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# Extract adjectives, nouns, and verbs from the dataset\n",
    "data['adjectives'] = data['Lemmatized_Comments'].apply(lambda x: extract_pos(x, adjective_tags))\n",
    "data['nouns'] = data['Lemmatized_Comments'].apply(lambda x: extract_pos(x, noun_tags))\n",
    "data['verbs'] = data['Lemmatized_Comments'].apply(lambda x: extract_pos(x, verb_tags))\n",
    "\n",
    "# Display the DataFrame with extracted parts of speech\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a913eb3b-8c43-4c9d-9786-758610bb82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\paris\\OneDrive\\thesis\\collect data\\extracted_pos_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333954b3-0aac-4869-8252-dece990bc372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
