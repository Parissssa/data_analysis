{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b753fcf3-1d79-42ac-a8f0-99a53e9defe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nLtk in c:\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\programs\\python\\python312\\lib\\site-packages (from nLtk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\programs\\python\\python312\\lib\\site-packages (from click->nLtk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nLtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c618dc-9eb9-4f14-9b1c-5690f99afbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12708173-cdd1-4ec5-a792-b683b9ae0230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download NLTK stopwords, punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457ee3f7-be11-4224-bfb6-871aa43a0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stop words\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38f0fea-00c1-4038-b573-0f97db03f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file and select only the 'Comments' column\n",
    "data = pd.read_csv(r'C:\\Users\\paris\\OneDrive\\thesis\\collect data\\Instagram-comments.csv')[['Comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7750bd7c-1328-48c3-8462-77f35dbecbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Comments\n",
      "0  So lush and looks very nourishing.. Perfect fo...\n",
      "1                                        So pretty üòç\n",
      "2                                                 üòçüòç\n",
      "3                                   Labour of love üòç\n",
      "4                                                üòçüòçüòç\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0479593-ec39-47e3-9a80-5e53136babbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\paris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization, reduce words to their base or root form\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5edf0d1a-d997-4c9d-bc7d-4085b94e6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comments  \\\n",
      "0    So lush and looks very nourishing.. Perfect fo...   \n",
      "1                                          So pretty üòç   \n",
      "2                                                   üòçüòç   \n",
      "3                                     Labour of love üòç   \n",
      "4                                                  üòçüòçüòç   \n",
      "..                                                 ...   \n",
      "494                              All the best brands ü©∑   \n",
      "495         @kelenthemelonn right?!?! We LOVE @lawless   \n",
      "496                         Woooo congrats, friends! ‚ù§   \n",
      "497                         Julep gluten free as well?   \n",
      "498   #LeapingBunnyCertified #CrueltyFree #julepbeauty   \n",
      "\n",
      "                                    Cleaned_Comments  \n",
      "0    looks nourishing perfect keeping lips soft cool  \n",
      "1                                             pretty  \n",
      "2                                                     \n",
      "3                                        labour love  \n",
      "4                                                     \n",
      "..                                               ...  \n",
      "494                                  all best brands  \n",
      "495                                   right ? ? love  \n",
      "496                                            woooo  \n",
      "497                              julep gluten free ?  \n",
      "498                leapingbunnycertified crueltyfree  \n",
      "\n",
      "[499 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define custom stop words excluding sentiment-critical words like \"not\"\n",
    "\n",
    "custom_stop_words = stop_words.union({\"hi\", \"day\", \"bar\", \"ill\", \"weather\", \"new\", \"sound\", \"see\", \"well\", \"im\", \"ive\", \n",
    " \"also\", \"youre\", \"win\", \"thats\", \"send\", \"dm\", \"mascara\", \"laurence\", \"nars\", \"pp\", \"ao\", \"de\", \"e\", \"xx\", \"xxx\", \"lush\", \n",
    " \"Shared\", \"happy birthday\", \"congrats\", \"lindo\", \"congratulations\", \"vraiment\", \"year\", \"spectator\", \"sport\", \"inbox\", \"website\",\n",
    " \"Lawless\", \"friends\", \"julepbeauty\"\n",
    " \n",
    "}).difference({\"able\", \"affected\", \"affecting\", \"affects\", \"after\", \"again\", \"against\", \"ain't\", \"all\", \"always\",\n",
    "    \"anyone\", \"anybody\", \"aren't\", \"awfully\", \"best\", \"better\", \"definitely\", \"end\", \"ending\", \"fairly\", \"will\", \"have this\",\n",
    "    \"hopefully\", \"however\", \"but\", \"importance\", \"important\", \"information\", \"instead\", \"interest\", \"isn't\", \"have product\",\n",
    "    \"less\", \"like\", \"liked\", \"mustn't\", \"necessary\", \"needn't\", \"ok\", \"okay\", \"shouldn't\", \"suggest\", \"don't\", \"needn\", \n",
    "    \"unfortunately\", \"use\", \"used\", \"useful\", \"using\", \"want\", \"wants\", \"wasn't\", \"won't\", \"wouldn't\", \"not\", \"no\", \"never\"}) \n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map Penn Treebank POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# Function to clean text \n",
    "def clean_text(text):\n",
    "#to handle non-string values by converting them to strings or skipping them\n",
    "    if not isinstance(text, str):  # Check if the input is not a string\n",
    "        return \"\"\n",
    "\n",
    "#1. remove URLs, Mentions-Tags-\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\s+|www\\s+', '', text)\n",
    "#2. remove special charachters and numbers,but keep question marks-sentiment-critical\n",
    "    text = re.sub(r'[^a-zA-Z\\s?]','',text)\n",
    "#3. convert to lowercase- Text normalization\n",
    "    text = text.lower()\n",
    "#4. Tokenizw and remove stop words\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_words = [word for word in tokens if word not in custom_stop_words]\n",
    "    # Join words back into one string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "#Apply function to each text in the DataFrame\n",
    "data['Cleaned_Comments'] = data['Comments'].apply(clean_text)\n",
    "print(data[['Comments', 'Cleaned_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6127252a-165f-40e6-b549-cb9d471b77c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itself', 'have', 'day', 'new', 'having', 's', 'with', 'send', 'because', 'same', 'about', 'theirs', 'once', 'its', 'few', 'over', 'you', 'he', 'so', 'o', 'weather', 'out', 'those', 'spectator', 'what', 'this', 'up', 'shouldn', 'then', 'xx', 'Lawless', 'themselves', 'are', \"you're\", 'did', 'further', 'there', 'only', 'didn', 'me', 'in', 'where', 'our', 'y', 'off', \"hasn't\", 'mascara', 'more', 'we', 'sound']\n"
     ]
    }
   ],
   "source": [
    "# Print the first few stop words to confirm\n",
    "print(list(custom_stop_words)[:50])  # Debugging: Check stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31bbc785-3371-49bf-adde-4864ee6c2a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "# Lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize cleaned text\n",
    "    pos_tags = pos_tag(tokens)  # Get POS tags\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)  # Return lemmatized text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856cf874-0a88-410c-9301-69034a9d5492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comments  \\\n",
      "0    So lush and looks very nourishing.. Perfect fo...   \n",
      "1                                          So pretty üòç   \n",
      "2                                                   üòçüòç   \n",
      "3                                     Labour of love üòç   \n",
      "4                                                  üòçüòçüòç   \n",
      "..                                                 ...   \n",
      "494                              All the best brands ü©∑   \n",
      "495         @kelenthemelonn right?!?! We LOVE @lawless   \n",
      "496                         Woooo congrats, friends! ‚ù§   \n",
      "497                         Julep gluten free as well?   \n",
      "498   #LeapingBunnyCertified #CrueltyFree #julepbeauty   \n",
      "\n",
      "                                    Cleaned_Comments  \\\n",
      "0    looks nourishing perfect keeping lips soft cool   \n",
      "1                                             pretty   \n",
      "2                                                      \n",
      "3                                        labour love   \n",
      "4                                                      \n",
      "..                                               ...   \n",
      "494                                  all best brands   \n",
      "495                                   right ? ? love   \n",
      "496                                            woooo   \n",
      "497                              julep gluten free ?   \n",
      "498                leapingbunnycertified crueltyfree   \n",
      "\n",
      "                          Lemmatized_Comments  \n",
      "0    look nourish perfect keep lips soft cool  \n",
      "1                                      pretty  \n",
      "2                                              \n",
      "3                                 labour love  \n",
      "4                                              \n",
      "..                                        ...  \n",
      "494                            all best brand  \n",
      "495                            right ? ? love  \n",
      "496                                     woooo  \n",
      "497                       julep gluten free ?  \n",
      "498         leapingbunnycertified crueltyfree  \n",
      "\n",
      "[499 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply Lemmatization \n",
    "data['Cleaned_Comments'] = data['Comments'].apply(clean_text)\n",
    "data['Lemmatized_Comments'] = data['Cleaned_Comments'].apply(lemmatize_text)\n",
    "\n",
    "print(data[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc9a6b1-c162-42b5-9f2a-1c43dbb2660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and keep only English comments\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False  # Default to non-English if detection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99800b6-a61e-4265-980c-34610e7ea740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows based on lemmatized comments: 45\n",
      "                                              Comments  \\\n",
      "67                                                 üëèüëèüëè   \n",
      "68                                                 üëèüëèüëè   \n",
      "69                                                  üëèüëè   \n",
      "70                                              ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "71                                              ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "100                    When will it be back in stock?üß°   \n",
      "101  It's a firm favourite with customers, and the ...   \n",
      "102                            Ohh need to try this! üòç   \n",
      "103  @ellastarbox Hi, they will be ready 27 September.   \n",
      "104       @willow_and_myrtle Thank you, can't wait! ‚ù§Ô∏è   \n",
      "114                                                 üëèüëè   \n",
      "127  So lush and looks very nourishing.. Perfect fo...   \n",
      "128                                                 üòçüòç   \n",
      "129                                                üòçüòçüòç   \n",
      "130                                         Wow ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "131                           Oh wow hun how amazing üòç   \n",
      "132  @kbsocialhub Thank you lovely, they sure are a...   \n",
      "158                                             ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "190                                                üòçüòçüòç   \n",
      "227                                                üòçüòçüòç   \n",
      "242                                                üëèüëèüëè   \n",
      "257                                                  üíô   \n",
      "259                                       @andrella744   \n",
      "260                                             Ended?   \n",
      "261  My mom @bmani2501 is my world. #ZeroWasteSkinc...   \n",
      "262                                           Shared üòç   \n",
      "263          @rithanya_radhakkrishnan @jishnucreations   \n",
      "264                          @benzi_vb @ash.laurence.1   \n",
      "270                                                üëèüëèüëè   \n",
      "285                                                  üòç   \n",
      "315                                                üëèüëèüëè   \n",
      "359                                                  üòç   \n",
      "360                                             ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "389                                                 üòçüòç   \n",
      "391                                               üòçüòçüòçüòç   \n",
      "397                                                üòçüòçüòç   \n",
      "407                                    So beautiful ‚ù§Ô∏è   \n",
      "412                                             ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "432                           Need to try this brand üòç   \n",
      "440                                                üôåüôåüôå   \n",
      "443                                                üôåüôåüôå   \n",
      "452                                               üòçüòçüòçüòç   \n",
      "476                                                 üòçüòç   \n",
      "477                                                üî•üî•üî•   \n",
      "490                                                 üòçüòç   \n",
      "\n",
      "                                      Cleaned_Comments  \\\n",
      "67                                                       \n",
      "68                                                       \n",
      "69                                                       \n",
      "70                                                       \n",
      "71                                                       \n",
      "100                                  will back stock ?   \n",
      "101  firm favourite customers scent blend devine re...   \n",
      "102                                       ohh need try   \n",
      "103                               will ready september   \n",
      "104                                    thank cant wait   \n",
      "114                                                      \n",
      "127    looks nourishing perfect keeping lips soft cool   \n",
      "128                                                      \n",
      "129                                                      \n",
      "130                                                wow   \n",
      "131                                 oh wow hun amazing   \n",
      "132                   thank lovely sure must all round   \n",
      "158                                                      \n",
      "190                                                      \n",
      "227                                                      \n",
      "242                                                      \n",
      "257                                                      \n",
      "259                                                      \n",
      "260                                            ended ?   \n",
      "261  mom world zerowasteskincare thebeanloop cheris...   \n",
      "262                                             shared   \n",
      "263                                                      \n",
      "264                                                      \n",
      "270                                                      \n",
      "285                                                      \n",
      "315                                                      \n",
      "359                                                      \n",
      "360                                                      \n",
      "389                                                      \n",
      "391                                                      \n",
      "397                                                      \n",
      "407                                          beautiful   \n",
      "412                                                      \n",
      "432                                     need try brand   \n",
      "440                                                      \n",
      "443                                                      \n",
      "452                                                      \n",
      "476                                                      \n",
      "477                                                      \n",
      "490                                                      \n",
      "\n",
      "                                   Lemmatized_Comments  \n",
      "67                                                      \n",
      "68                                                      \n",
      "69                                                      \n",
      "70                                                      \n",
      "71                                                      \n",
      "100                                  will back stock ?  \n",
      "101  firm favourite customer scent blend devine rea...  \n",
      "102                                       ohh need try  \n",
      "103                               will ready september  \n",
      "104                                    thank cant wait  \n",
      "114                                                     \n",
      "127           look nourish perfect keep lips soft cool  \n",
      "128                                                     \n",
      "129                                                     \n",
      "130                                                wow  \n",
      "131                                   oh wow hun amaze  \n",
      "132                   thank lovely sure must all round  \n",
      "158                                                     \n",
      "190                                                     \n",
      "227                                                     \n",
      "242                                                     \n",
      "257                                                     \n",
      "259                                                     \n",
      "260                                              end ?  \n",
      "261  mom world zerowasteskincare thebeanloop cheris...  \n",
      "262                                              share  \n",
      "263                                                     \n",
      "264                                                     \n",
      "270                                                     \n",
      "285                                                     \n",
      "315                                                     \n",
      "359                                                     \n",
      "360                                                     \n",
      "389                                                     \n",
      "391                                                     \n",
      "397                                                     \n",
      "407                                          beautiful  \n",
      "412                                                     \n",
      "432                                     need try brand  \n",
      "440                                                     \n",
      "443                                                     \n",
      "452                                                     \n",
      "476                                                     \n",
      "477                                                     \n",
      "490                                                     \n"
     ]
    }
   ],
   "source": [
    "# Identify duplicate rows\n",
    "duplicates = data[data.duplicated()]\n",
    "\n",
    "print(f\"Number of duplicate rows based on lemmatized comments: {duplicates.shape[0]}\")\n",
    "print(duplicates[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f21b06e-ee2d-48cc-9bcb-f69daa183f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cleaned DataFrame (after removing duplicates but keeping single-word comments): (482, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates but keep single-word comments\n",
    "data_cleaned = data[~(data.duplicated(subset=['Lemmatized_Comments']) & \n",
    "                      (data['Lemmatized_Comments'].str.split().str.len() > 1))]\n",
    "\n",
    "# Check the shape of the cleaned DataFrame\n",
    "print(f\"Shape of the cleaned DataFrame (after removing duplicates but keeping single-word comments): {data_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d2acb3c-7543-4673-a91d-ea2352b5b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comments  \\\n",
      "0    So lush and looks very nourishing.. Perfect fo...   \n",
      "1                                          So pretty üòç   \n",
      "3                                     Labour of love üòç   \n",
      "5                                           Wow ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "6                           Oh this is soooo pretty! ü§ç   \n",
      "..                                                 ...   \n",
      "494                              All the best brands ü©∑   \n",
      "495         @kelenthemelonn right?!?! We LOVE @lawless   \n",
      "496                         Woooo congrats, friends! ‚ù§   \n",
      "497                         Julep gluten free as well?   \n",
      "498   #LeapingBunnyCertified #CrueltyFree #julepbeauty   \n",
      "\n",
      "                                    Cleaned_Comments  \\\n",
      "0    looks nourishing perfect keeping lips soft cool   \n",
      "1                                             pretty   \n",
      "3                                        labour love   \n",
      "5                                                wow   \n",
      "6                                    oh soooo pretty   \n",
      "..                                               ...   \n",
      "494                                  all best brands   \n",
      "495                                   right ? ? love   \n",
      "496                                            woooo   \n",
      "497                              julep gluten free ?   \n",
      "498                leapingbunnycertified crueltyfree   \n",
      "\n",
      "                          Lemmatized_Comments  \n",
      "0    look nourish perfect keep lips soft cool  \n",
      "1                                      pretty  \n",
      "3                                 labour love  \n",
      "5                                         wow  \n",
      "6                             oh soooo pretty  \n",
      "..                                        ...  \n",
      "494                            all best brand  \n",
      "495                            right ? ? love  \n",
      "496                                     woooo  \n",
      "497                       julep gluten free ?  \n",
      "498         leapingbunnycertified crueltyfree  \n",
      "\n",
      "[416 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'Lemmatized_Comments' is empty or only contains whitespace, but avoid reformatting issues\n",
    "data = data[data['Lemmatized_Comments'].astype(bool)]  # Keep only rows with non-empty strings\n",
    "\n",
    "# Check the resulting DataFrame to ensure formatting is preserved\n",
    "print(data[['Comments', 'Cleaned_Comments', 'Lemmatized_Comments']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "115e000e-bd3b-429c-aebf-c0bad221cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\paris\\OneDrive\\thesis\\collect data\\Cleaned_Instagram-comments3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f654b2c9-57de-4be3-85b6-f6e13722785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comments  \\\n",
      "0    So lush and looks very nourishing.. Perfect fo...   \n",
      "1                                          So pretty üòç   \n",
      "3                                     Labour of love üòç   \n",
      "5                                           Wow ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è   \n",
      "6                           Oh this is soooo pretty! ü§ç   \n",
      "..                                                 ...   \n",
      "494                              All the best brands ü©∑   \n",
      "495         @kelenthemelonn right?!?! We LOVE @lawless   \n",
      "496                         Woooo congrats, friends! ‚ù§   \n",
      "497                         Julep gluten free as well?   \n",
      "498   #LeapingBunnyCertified #CrueltyFree #julepbeauty   \n",
      "\n",
      "                                    Cleaned_Comments  \\\n",
      "0    looks nourishing perfect keeping lips soft cool   \n",
      "1                                             pretty   \n",
      "3                                        labour love   \n",
      "5                                                wow   \n",
      "6                                    oh soooo pretty   \n",
      "..                                               ...   \n",
      "494                                  all best brands   \n",
      "495                                   right ? ? love   \n",
      "496                                            woooo   \n",
      "497                              julep gluten free ?   \n",
      "498                leapingbunnycertified crueltyfree   \n",
      "\n",
      "                          Lemmatized_Comments                adjectives  \\\n",
      "0    look nourish perfect keep lips soft cool  [nourish, perfect, soft]   \n",
      "1                                      pretty                        []   \n",
      "3                                 labour love                        []   \n",
      "5                                         wow                        []   \n",
      "6                             oh soooo pretty                        []   \n",
      "..                                        ...                       ...   \n",
      "494                            all best brand                    [best]   \n",
      "495                            right ? ? love                        []   \n",
      "496                                     woooo                        []   \n",
      "497                       julep gluten free ?                    [free]   \n",
      "498         leapingbunnycertified crueltyfree                        []   \n",
      "\n",
      "                  nouns                    verbs  \n",
      "0    [look, lips, cool]                   [keep]  \n",
      "1                    []                       []  \n",
      "3              [labour]                   [love]  \n",
      "5                 [wow]                       []  \n",
      "6       [soooo, pretty]                       []  \n",
      "..                  ...                      ...  \n",
      "494             [brand]                       []  \n",
      "495                  []                   [love]  \n",
      "496             [woooo]                       []  \n",
      "497     [julep, gluten]                       []  \n",
      "498       [crueltyfree]  [leapingbunnycertified]  \n",
      "\n",
      "[416 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract words by part of speech (POS)\n",
    "def extract_pos(text, pos_list):\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tagged_tokens = pos_tag(tokens)  # Part-of-speech tagging\n",
    "    return [word for word, tag in tagged_tokens if tag in pos_list]\n",
    "\n",
    "# Define POS tags for adjectives, nouns, and verbs\n",
    "adjective_tags = ['JJ', 'JJR', 'JJS']\n",
    "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# Extract adjectives, nouns, and verbs from the dataset\n",
    "data['adjectives'] = data['Lemmatized_Comments'].apply(lambda x: extract_pos(x, adjective_tags))\n",
    "data['nouns'] = data['Lemmatized_Comments'].apply(lambda x: extract_pos(x, noun_tags))\n",
    "data['verbs'] = data['Lemmatized_Comments'].apply(lambda x: extract_pos(x, verb_tags))\n",
    "\n",
    "# Display the DataFrame with extracted parts of speech\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeabb8a2-54ff-4767-80aa-7e4ec93da1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
